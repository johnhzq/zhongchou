{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inPutDir = 'inPut/'\n",
    "outPutDir = 'outPut/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pynlpir\n",
    "pynlpir.open(encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 导入用户词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDict = inPutDir+\"myDict.txt\"\n",
    "pynlpir.nlpir.ImportUserDict(myDict.encode('utf8'))# 导入用户词典\n",
    "# pynlpir.nlpir.AddUserWord('电子商务'.encode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 生成停用词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1739\n",
      "1751\n"
     ]
    }
   ],
   "source": [
    "StopwordsFilename_1 = inPutDir+\"stopwords.txt\"\n",
    "stopwords = []\n",
    "for line in open(StopwordsFilename_1,encoding='utf8').readlines():# 生成停用词列表\n",
    "    stopwords.append(line.strip())# .strip()去掉换行符\n",
    "print(len(stopwords))\n",
    "\n",
    "StopwordsFilename_2 = inPutDir+\"myStopwords.txt\"\n",
    "for line in open(StopwordsFilename_2,encoding='utf8').readlines():# 生成停用词列表\n",
    "    stopwords.append(line.strip())# .strip()去掉换行符\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、自定义分词函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pynlpir_segment(mytext,stopwords,pos_tagging=True):\n",
    "    seg_list = pynlpir.segment(mytext,pos_tagging) ## 分词\n",
    "#     print(seg_list)\n",
    "    nav_seg_list=[]\n",
    "    if pos_tagging:\n",
    "        ##过滤特定词性的词和单字\n",
    "        for (word,flag) in seg_list:\n",
    "            if flag == \"noun\" or flag == \"verb\" or flag == \"adjective\":\n",
    "                if len(word)>1:\n",
    "                    nav_seg_list.append(word)\n",
    "    else:\n",
    "        for word in seg_list:            \n",
    "            if len(word)>1:\n",
    "                nav_seg_list.append(word)\n",
    "    removed_seg_list = []\n",
    "    ##过滤停用词\n",
    "    for w in nav_seg_list:\n",
    "        if w not in stopwords:\n",
    "            removed_seg_list.append(w)\n",
    "    return \" \".join(removed_seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "落实 财政部 商务部 扶贫办 开展 电子商务 农村 综合 示范 工作 示范县 带头 作用\n",
      "----------------------------------------------------------------------------------------------------\n",
      "落实 财政部 商务部 扶贫办 开展 2016年 电子商务 农村 综合 示范 工作 示范县 带头 作用\n"
     ]
    }
   ],
   "source": [
    "mytext = '为认真落实《财政部、商务部、扶贫办关于开展2016年电子商务进农村综合示范的工作通,示范县起带头作用'\n",
    "print(pynlpir_segment(mytext,stopwords)) ##过滤特定词性的词\n",
    "print('-'*100)\n",
    "print(pynlpir_segment(mytext,stopwords,False)) ##不过滤特定词性的词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1137, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>_no</th>\n",
       "      <th>_EMW</th>\n",
       "      <th>label</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433150298</td>\n",
       "      <td>110100</td>\n",
       "      <td>1</td>\n",
       "      <td>果蔬食品</td>\n",
       "      <td>一人一元，寻找身边放心大米为了我们和家人能吃上放心大米,将筹到的资金,委托中国首家放心食品甄...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1435059060</td>\n",
       "      <td>110100</td>\n",
       "      <td>1</td>\n",
       "      <td>茶酒饮品</td>\n",
       "      <td>《不知名茶3》长白山脉已被我们承包，只差你这个森林之王从1000座山到整片森林，包山（林）种...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1432894713</td>\n",
       "      <td>110100</td>\n",
       "      <td>1</td>\n",
       "      <td>果蔬食品</td>\n",
       "      <td>【黑鸡枞教父众筹】聆听吃出的健康传奇金泓言用他30年的心血，投入到昔日的皇家贡品黑鸡枞的研发...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1478257937</td>\n",
       "      <td>230000</td>\n",
       "      <td>2</td>\n",
       "      <td>五常#稻花香#三株稻</td>\n",
       "      <td>舌尖上的五常稻花香米，“三株稻”品质良心米（众筹第二期）三年，我们联合了80余家稻农，精耕了...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1445414025</td>\n",
       "      <td>230000</td>\n",
       "      <td>2</td>\n",
       "      <td>生态养殖#五常大米#正宗大米</td>\n",
       "      <td>舌尖上的五常稻花香米，“三株稻”品质良心米带动乡里乡亲致富的同时，把这正宗的五常米，直接送到...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time     _no  _EMW           label  \\\n",
       "0  1433150298  110100     1            果蔬食品   \n",
       "1  1435059060  110100     1            茶酒饮品   \n",
       "2  1432894713  110100     1            果蔬食品   \n",
       "3  1478257937  230000     2      五常#稻花香#三株稻   \n",
       "4  1445414025  230000     2  生态养殖#五常大米#正宗大米   \n",
       "\n",
       "                                            document  \n",
       "0  一人一元，寻找身边放心大米为了我们和家人能吃上放心大米,将筹到的资金,委托中国首家放心食品甄...  \n",
       "1  《不知名茶3》长白山脉已被我们承包，只差你这个森林之王从1000座山到整片森林，包山（林）种...  \n",
       "2  【黑鸡枞教父众筹】聆听吃出的健康传奇金泓言用他30年的心血，投入到昔日的皇家贡品黑鸡枞的研发...  \n",
       "3  舌尖上的五常稻花香米，“三株稻”品质良心米（众筹第二期）三年，我们联合了80余家稻农，精耕了...  \n",
       "4  舌尖上的五常稻花香米，“三株稻”品质良心米带动乡里乡亲致富的同时，把这正宗的五常米，直接送到...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataFile = inPutDir+'DocumentS.txt'\n",
    "f = open(dataFile,encoding='utf8')\n",
    "DocumentS = pd.read_table(f,sep='|')\n",
    "f.close()\n",
    "print(DocumentS.shape)\n",
    "DocumentS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocumentS[\"doc_cutted_T\"] = DocumentS.document.apply(lambda mytext:pynlpir_segment(mytext,stopwords))\n",
    "DocumentS[\"doc_cutted_F\"] = DocumentS.document.apply(lambda mytext:pynlpir_segment(mytext,stopwords,False))\n",
    "DocumentS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DocumentS = DocumentS.astype(str)\n",
    "filename = outPutDir+'DocumentS_cutted.txt'\n",
    "file = open(filename, \"a\",encoding='utf8')  ## \"w\"表示重新写\n",
    "file.write('|'.join(DocumentS.columns)+'\\n')\n",
    "for i in range(DocumentS.shape[0]):\n",
    "    line = '|'.join(list(DocumentS.iloc[i,:]))\n",
    "    file.write(line+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# @\n",
    "- 读 txt 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = outPutDir+'DocumentS_cutted.txt'\n",
    "file = open(filename,encoding='utf8')\n",
    "temp = pd.read_table(file,sep='|')\n",
    "file.close()\n",
    "print(temp.shape)\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
